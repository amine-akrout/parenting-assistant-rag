# ğŸ¤– AI-Powered Parenting Assistant

## ğŸ“Œ Overview
The **AI-Powered Parenting Assistant** is an intelligent chatbot designed to provide safe, reliable, and empathetic responses to parenting-related questions. Built using **LangChain**, **LLM Guard**, and **FastAPI**, the system leverages **Retrieval-Augmented Generation (RAG)** to enhance its responses with relevant knowledge from curated sources.

The chatbot also integrates **advanced content filtering** to ensure the safety and appropriateness of responses, making it a powerful demonstration of AI engineering best practices.

---

## ğŸš€ Features
### âœ… **Retrieval-Augmented Generation (RAG) System**
- Uses **FAISS vector search** combined with **BM25 retrieval** for accurate contextual knowledge retrieval.
- Implements **OpenVINO Reranker** for document ranking and relevance filtering.

### ğŸ›¡ **AI Safety with LLM Guard**
- **Input Filtering**: Blocks **toxic, political, religious, and self-harm topics** using `llm_guard`.
- **Output Validation**: Scans model responses for **language safety, sensitivity, and relevance**.
- **Routing Mechanism**: Ensures inappropriate queries are handled gracefully.

### âš¡ **High-Performance AI Pipeline**
- **LangChain Pipelines**: Modular **runnable chains** for seamless data flow.
- **Prompt Engineering**: Optimized **few-shot prompting** for structured AI outputs.
- **FastAPI Backend**: Fully asynchronous **REST API** for real-time chatbot interactions.
- **Langfuse Monitoring**: Enables **real-time LLM performance tracking**.

### ğŸ”¥ **Job-Worthy Tech Stack**
âœ… **AI & LLMs**: OpenAI GPT Models, LangChain, HuggingFace Embeddings  
âœ… **Data Retrieval**: FAISS, BM25, OpenVINO Reranker  
âœ… **Safety**: LLM Guard (Input & Output Filtering)  
âœ… **Backend API**: FastAPI, Pydantic, Loguru  
âœ… **Monitoring**: Langfuse Callbacks  
âœ… **Dockerized Pipeline**: Multi-stage AI data processing stack  

---

## ğŸ“ Project Structure
```
ğŸ“‚ src/
 â”œâ”€â”€ core/
 â”‚   â”œâ”€â”€ chatbot.py       # Main AI chatbot pipeline (LLM + Retrieval + Guard)
 â”‚   â”œâ”€â”€ embedding.py     # FAISS/BM25 index generation
 â”‚   â”œâ”€â”€ evaluation.py    # AI performance and accuracy checks
 â”‚   â”œâ”€â”€ filters.py       # LLM Guard configurations for input/output scanning
 â”‚â”€â”€ config.py            # Application settings & environment variables
 â”‚
 â”œâ”€â”€ api/
 â”‚   â”œâ”€â”€ routers/
 â”‚   â”‚   â””â”€â”€ chat.py      # FastAPI routes for chatbot integration
 â”‚   â”‚â”€â”€ __init__.py      # API initializer
 â”‚   â””â”€â”€ main.py          # FastAPI app initialization
 â”‚
 â”œâ”€â”€ data/
 â”‚   â”œâ”€â”€ clean_data.py    # Data preprocessing scripts
 â”‚
 â”œâ”€â”€ monitoring/
 â”‚   â””â”€â”€ monitoring.py    # Langfuse monitoring setup
 â”‚
 â”œâ”€â”€ Dockerfile           # Containerization setup
 â”œâ”€â”€ docker-compose.yml   # Multi-container deployment setup
 â”œâ”€â”€ requirements.txt     # Python dependencies
 â”œâ”€â”€ README.md            # Project documentation (this file)
```

---

## ğŸ— Setup & Installation
### 1ï¸âƒ£ **Clone the Repository**
```bash
git clone https://github.com/amine-akrout/parenting_assistant_rag.git
cd parenting_assistant_rag
```

### 2ï¸âƒ£ **Create Virtual Environment & Install Dependencies**
```bash
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
pip install -r requirements.txt
```

### 3ï¸âƒ£ **Set Up Environment Variables**
Create a `.env` file in the root directory:
```ini
POSTGRES_USER=your_db_user
POSTGRES_PASSWORD=your_db_password
POSTGRES_DB=your_db_name
OPENAI_API_KEY=your-openai-key
FAISS_INDEX_PATH=./data/faiss_index.faiss
BM25_INDEX_PATH=./data/bm25_index.pkl
CROSS_ENCODER_MODEL_NAME=your-cross-encoder-model
LANGFUSE_PUBLIC_KEY=your-langfuse-public-key
LANGFUSE_SECRET_KEY=your-langfuse-secret-key
```

### 4ï¸âƒ£ **Run the API Server**
```bash
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
```

### 5ï¸âƒ£ **Test the API**
Open **Swagger UI** to test endpoints:  
ğŸ“Œ `http://localhost:8000/docs`

---

## ğŸ³ Dockerized Workflow
This project includes a **Dockerized data pipeline** that prepares, indexes, and serves the chatbot API.  

### 1ï¸âƒ£ **Start All Services**
```bash
docker-compose up --build
```
### 2ï¸âƒ£ **Stop All Services**
```bash
docker-compose down
```

### **Docker Services Overview**
- **`langfuse-server`** â†’ Logs chatbot requests/responses for monitoring
- **`db`** â†’ PostgreSQL database for logging and indexing
- **`clean-data`** â†’ Prepares and cleans datasets before indexing
- **`preprocess`** â†’ Creates FAISS/BM25 indexes for fast retrieval
- **`chatbot-api`** â†’ Runs the **FastAPI** chatbot backend


---

## ğŸ“ˆ Performance Monitoring (Langfuse)
Enable **Langfuse** for real-time monitoring:
```bash
export LANGFUSE_API_KEY=your-langfuse-key
```
View logs on **Langfuse Dashboard**.

---

## ğŸ¯ Future Enhancements
âœ… Add **multi-turn conversations** (memory support)  
âœ… Implement **whisper model for speech-to-text**  
âœ… Add **more retrieval sources (vector & hybrid search)**  
âœ… Integrate **more LLM Guard features** (e.g., sentiment analysis)  
âœ… Add **Unit Tests** for API endpoints and core functions


---

## ğŸ¤ Contributing
Feel free to **open an issue** or **submit a pull request**.

---


